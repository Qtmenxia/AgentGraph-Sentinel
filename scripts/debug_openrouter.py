import os
import requests
import sys
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

# 1. å¼ºåˆ¶åŠ è½½ .env æ–‡ä»¶
# å‡è®¾è„šæœ¬åœ¨ scripts/ ç›®å½•ä¸‹ï¼Œ.env åœ¨ä¸Šä¸€çº§ç›®å½•
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
env_path = os.path.join(project_root, '.env')

print(f"ğŸ“‚ æ­£åœ¨å°è¯•åŠ è½½ .env æ–‡ä»¶è·¯å¾„: {env_path}")
load_dotenv(env_path)

def test_environment():
    print("\n" + "="*50)
    print("TEST 1: ç¯å¢ƒå˜é‡æ£€æŸ¥")
    print("="*50)
    
    api_key = os.getenv("OPENROUTER_API_KEY")
    base_url = os.getenv("OPENROUTER_BASE_URL", "https://openrouter.ai/api/v1")
    model = os.getenv("OPENROUTER_MODEL", "openai/gpt-3.5-turbo")

    if not api_key:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ° OPENROUTER_API_KEYã€‚è¯·æ£€æŸ¥ .env æ–‡ä»¶ï¼")
        return False, None, None, None
    
    # æ‰“å°éƒ¨åˆ† Key ä»¥éªŒè¯è¯»å–æ˜¯å¦æ­£ç¡®ï¼ˆé˜²æ­¢è¯»å–åˆ°ç©ºå­—ç¬¦ä¸²ï¼‰
    masked_key = f"{api_key[:6]}...{api_key[-4:]}" if len(api_key) > 10 else "***"
    print(f"âœ… API Key å·²åŠ è½½: {masked_key}")
    print(f"âœ… Base URL: {base_url}")
    print(f"âœ… Model: {model}")
    return True, api_key, base_url, model

def test_raw_http(api_key, base_url, model):
    print("\n" + "="*50)
    print("TEST 2: åŸç”Ÿ HTTP è¯·æ±‚æµ‹è¯• (ç»•è¿‡ LangChain)")
    print("="*50)
    
    headers = {
        "Authorization": f"Bearer {api_key}",
        "HTTP-Referer": "http://localhost:8501", # OpenRouter è¦æ±‚çš„å¤´
        "X-Title": "Debug-Script",
        "Content-Type": "application/json"
    }
    
    # OpenRouter çš„ Chat å®Œæˆæ¥å£é€šå¸¸æ˜¯ /chat/completions
    # å¦‚æœ base_url ç»“å°¾æœ‰ /v1ï¼Œåˆ™æ‹¼æ¥ /chat/completions
    target_url = base_url.rstrip('/') + "/chat/completions"
    
    payload = {
        "model": model,
        "messages": [{"role": "user", "content": "Say 'Hello' if you can hear me."}],
    }

    print(f"ğŸ“¡ å‘é€è¯·æ±‚åˆ°: {target_url}")
    try:
        response = requests.post(target_url, headers=headers, json=payload, timeout=10)
        
        print(f"ğŸ”„ HTTP çŠ¶æ€ç : {response.status_code}")
        
        if response.status_code == 200:
            print(f"âœ… å“åº”æˆåŠŸ: {response.json()['choices'][0]['message']['content']}")
            return True
        else:
            print(f"âŒ è¯·æ±‚å¤±è´¥. å“åº”å†…å®¹:\n{response.text}")
            return False
    except Exception as e:
        print(f"âŒ è¿æ¥å¼‚å¸¸: {e}")
        return False

def test_langchain(api_key, base_url, model):
    print("\n" + "="*50)
    print("TEST 3: LangChain é›†æˆæµ‹è¯•")
    print("="*50)
    
    try:
        # æ¨¡æ‹Ÿ agent_executor.py ä¸­çš„åˆå§‹åŒ–æ–¹å¼
        llm = ChatOpenAI(
            openai_api_key=api_key,      # æ³¨æ„ï¼šLangChainå†…éƒ¨å‚æ•°åé€šå¸¸æ˜¯ openai_api_key
            openai_api_base=base_url,    # æ³¨æ„ï¼šæ—§ç‰ˆå¯èƒ½ç”¨ openai_api_baseï¼Œæ–°ç‰ˆç”¨ base_url
            model_name=model,
            temperature=0,
            default_headers={
                "HTTP-Referer": "http://localhost:8501",
                "X-Title": "AgentGraph-Debug"
            }
        )
        
        print("ğŸ¤– æ­£åœ¨è°ƒç”¨ LLM invoke()...")
        response = llm.invoke([HumanMessage(content="Test connection.")])
        print(f"âœ… LangChain è°ƒç”¨æˆåŠŸ: {response.content}")
        return True
    except Exception as e:
        print(f"âŒ LangChain è°ƒç”¨å¤±è´¥: {e}")
        # æ‰“å°æ›´å¤šè°ƒè¯•ä¿¡æ¯
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success_env, key, url, model = test_environment()
    
    if success_env:
        success_http = test_raw_http(key, url, model)
        
        if success_http:
            test_langchain(key, url, model)
        else:
            print("\nâš ï¸ è·³è¿‡ Test 3ï¼Œå› ä¸ºåŸç”Ÿ HTTP è¯·æ±‚å·²å¤±è´¥ã€‚è¯·å…ˆè§£å†³ Key æˆ–ç½‘ç»œé—®é¢˜ã€‚")
